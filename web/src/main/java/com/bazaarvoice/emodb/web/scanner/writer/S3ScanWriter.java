package com.bazaarvoice.emodb.web.scanner.writer;

import com.amazonaws.AmazonClientException;
import com.amazonaws.event.ProgressEvent;
import com.amazonaws.event.ProgressListener;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.model.AmazonS3Exception;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.PutObjectResult;
import com.amazonaws.util.BinaryUtils;
import com.bazaarvoice.emodb.web.scanner.ScanUploadService;
import com.codahale.metrics.MetricRegistry;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.base.Optional;
import com.google.common.base.Strings;
import com.google.common.collect.Maps;
import com.google.common.collect.Sets;
import com.google.common.hash.Hashing;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.SettableFuture;
import com.google.inject.Inject;
import com.google.inject.assistedinject.Assisted;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.net.URI;
import java.time.Duration;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.Future;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import static com.google.common.base.Preconditions.checkArgument;
import static java.util.Objects.requireNonNull;

/**
 * ScanWriter implementation which uploads files to S3.
 */
public class S3ScanWriter extends TemporaryFileScanWriter {
    private static final Duration DEFAULT_RETRY_DELAY = Duration.ofSeconds(5);
    private static final int MAX_RETRIES = 3;

    private static final Logger _log = LoggerFactory.getLogger(S3ScanWriter.class);

    private final AmazonS3 _amazonS3;
    private final ScheduledExecutorService _uploadService;
    private final Set<ActiveUpload> _activeUploads = Sets.newSetFromMap(Maps.<ActiveUpload, Boolean>newConcurrentMap());
    private Duration _retryDelay = DEFAULT_RETRY_DELAY;

    @Inject
    public S3ScanWriter(@Assisted int taskId, @Assisted URI baseUri, @Assisted Optional<Integer> maxOpenShards,
                        MetricRegistry metricRegistry, AmazonS3Provider amazonS3Provider,
                        @ScanUploadService ScheduledExecutorService uploadService, ObjectMapper objectMapper) {
        super("s3", taskId, baseUri, Compression.GZIP, metricRegistry, maxOpenShards, objectMapper);

        requireNonNull(amazonS3Provider, "amazonS3Provider is required");
        String bucket = baseUri.getHost();
        checkArgument(!Strings.isNullOrEmpty(bucket), "bucket is required");
        _amazonS3 = amazonS3Provider.getS3ClientForBucket(bucket);
        _uploadService = requireNonNull(uploadService, "uploadService is required");
    }

    public void setRetryDelay(Duration retryDelay) {
        _retryDelay = retryDelay;
    }

    @Override
    protected ListenableFuture<?> transfer(TransferKey transferKey, URI uri, File file) {
        ActiveUpload activeUpload = new ActiveUpload(transferKey, uri, file);
        ActiveUploadRunner runner = new ActiveUploadRunner(activeUpload);
        return runner.start();
    }

    /**
     * Callable class to perform an active upload and get metadata about the progress.
     */
    private class ActiveUpload {
        private final TransferKey _transferKey;
        private final URI _uri;
        private final String _bucket;
        private final String _key;
        private final File _file;
        private int _attempts = 0;
        private long _bytesTransferred = 0;
        private Future<?> _uploadFuture;
        private SettableFuture<String> _resultFuture;

        ActiveUpload(TransferKey transferKey, URI uri, File file) {
            _transferKey = transferKey;
            _uri = uri;
            _bucket = uri.getHost();
            _key = getKeyFromPath(uri);
            _file = file;
        }

        /**
         * Starts an asynchronous upload and returns a ListenableFuture for handling the result.
         */
        synchronized ListenableFuture<String> upload() {
            // Reset values from possible prior attempt
            _attempts += 1;
            _bytesTransferred = 0;

            // Separate the future returned to the caller from the future generated by submitting the
            // putObject request.  If the writer is closed then uploadFuture may be canceled before it executes,
            // in which case it may not trigger any callbacks.  To ensure there is always a callback resultFuture is
            // tracked independently and, in the event that the upload is aborted, gets set on abort().
            _resultFuture = SettableFuture.create();

            _uploadFuture = _uploadService.submit(new Runnable() {
                @Override
                public void run() {
                    try {
                        ProgressListener progressListener = new ProgressListener() {
                            @Override
                            public void progressChanged(ProgressEvent progressEvent) {
                                // getBytesTransferred() returns zero for all events not pertaining to the file transfer
                                _bytesTransferred += progressEvent.getBytesTransferred();
                            }
                        };

                        PutObjectRequest putObjectRequest = new PutObjectRequest(_bucket, _key, _file);
                        putObjectRequest.setGeneralProgressListener(progressListener);
                        PutObjectResult result = _amazonS3.putObject(putObjectRequest);
                        _resultFuture.set(result.getETag());
                    } catch (Throwable t) {
                        _resultFuture.setException(t);
                    }
                }
            });

            return _resultFuture;
        }

        synchronized TransferStatus getTransferStatus() {
            return new TransferStatus(_transferKey, _file.length(), _attempts, _bytesTransferred);
        }

        synchronized void abort() {
            // uploadFuture and resultFuture are set atomically so only need to null check for one.
            if (_uploadFuture != null) {
                try {
                    _resultFuture.setException(new IOException("Writer closed"));
                    _uploadFuture.cancel(true);
                } finally {
                    _uploadFuture = null;
                    _resultFuture = null;
                }
            }
        }

        private URI getUri() {
            return _uri;
        }

        private File getFile() {
            return _file;
        }

        private int getAttempts() {
            return _attempts;
        }

        @Override
        public boolean equals(Object o) {
            if (this == o) {
                return true;
            }
            if (!(o instanceof ActiveUpload)) {
                return false;
            }

            ActiveUpload that = (ActiveUpload) o;
            return _transferKey.equals(that._transferKey);
        }

        @Override
        public int hashCode() {
            return _transferKey.hashCode();
        }
    }

    /**
     * Runnable for executing ActiveUploads and handling asynchronous callbacks.
     */
    private class ActiveUploadRunner implements Runnable, FutureCallback<String> {
        private final ActiveUpload _activeUpload;
        private final SettableFuture<String> _finalFuture;

        private ActiveUploadRunner(ActiveUpload activeUpload) {
            _activeUpload = activeUpload;
            _finalFuture = SettableFuture.create();
        }

        private ListenableFuture<String> start() {
            // Since run() asynchronously starts the upload process starting the running is effectively
            // just calling run() synchronously and returning the final future.
            run();
            return _finalFuture;
        }

        @Override
        public void run() {
            if (_closed) {
                _finalFuture.setException(new IOException("Writer closed"));
            }
            ListenableFuture<String> attemptFuture = _activeUpload.upload();
            Futures.addCallback(attemptFuture, this);
        }

        @Override
        public void onSuccess(String result) {
            _log.debug("Transferring file: id={}, file={}, uri={}... DONE",
                    _taskId, _activeUpload.getFile(), _activeUpload.getUri());
            _activeUploads.remove(_activeUpload);
            _finalFuture.set(result);
        }

        @Override
        public void onFailure(Throwable t) {
            // If the writer is closed then always propagate the exception immediately and
            // don't bother logging a warning.
            if (!_closed) {
                if (_activeUpload.getAttempts() < MAX_RETRIES) {
                    try {
                        _uploadService.schedule(this, _retryDelay.toMillis(), TimeUnit.MILLISECONDS);
                        _log.debug("Transferring file failed, will retry: id={}, file={}, uri={}...",
                                _taskId, _activeUpload.getFile(), _activeUpload.getUri(), t);
                        return;
                    } catch (Throwable t2) {
                        // If the reschedule failed for any reason just fall through
                        // and propagate the exception now.
                    }
                }

                _log.warn("Transferring file failed, no more retries: id={}, file={}, uri={}",
                        _taskId, _activeUpload.getFile(), _activeUpload.getUri(), t);
            }

            _activeUploads.remove(_activeUpload);
            _finalFuture.setException(t);
        }
    }

    @Override
    protected Map<TransferKey, TransferStatus> getStatusForActiveTransfers() {
        Map<TransferKey, TransferStatus> statusMap = Maps.newHashMap();
        for (ActiveUpload activeUpload : _activeUploads) {
            TransferStatus transferStatus = activeUpload.getTransferStatus();
            statusMap.put(transferStatus.getKey(), transferStatus);
        }
        return statusMap;
    }

    @Override
    protected boolean writeScanCompleteFile(URI fileUri, byte[] contents)
            throws IOException {
        String bucket = fileUri.getHost();
        String key = getKeyFromPath(fileUri);

        try {
            // The following will throw an exception unless the file already exists
            _amazonS3.getObjectMetadata(bucket, key);
            return false;
        } catch (AmazonS3Exception e) {
            if (e.getStatusCode() != Response.Status.NOT_FOUND.getStatusCode()) {
                // Expected case is not found, meaning the file does not exist
                // All other cases are some unexpected error
                throw new IOException(e);
            }
        }

        uploadContents(bucket, key, contents);
        return true;
    }

    @Override
    protected void writeLatestFile(URI fileUri, byte[] contents)
            throws IOException {
        String bucket = fileUri.getHost();
        String key = getKeyFromPath(fileUri);
        uploadContents(bucket, key, contents);
    }

    private String getKeyFromPath(URI uri) {
        // S3 does not use leading slashes
        String path = uri.getPath();
        if (path.startsWith("/")) {
            return path.substring(1);
        }
        return path;
    }

    private void uploadContents(String bucket, String key, byte[] contents)
            throws IOException {
        int failures = 0;
        boolean uploaded = false;
        while (!uploaded) {
            ObjectMetadata objectMetadata = new ObjectMetadata();
            objectMetadata.setContentType(MediaType.TEXT_PLAIN);
            objectMetadata.setContentLength(contents.length);
            objectMetadata.setContentMD5(BinaryUtils.toBase64(Hashing.md5().hashBytes(contents).asBytes()));

            try {
                _amazonS3.putObject(
                        new PutObjectRequest(bucket, key, new ByteArrayInputStream(contents), objectMetadata));
                uploaded = true;
            } catch (AmazonClientException e) {
                if (++failures == MAX_RETRIES) {
                    throw new IOException(e);
                }
                try {
                    Thread.sleep(_retryDelay.toMillis());
                } catch (InterruptedException e2) {
                    // Stop retrying and propagate the original exception
                    throw new IOException(e);
                }
            }
        }
    }

    @Override
    public void close() {
        super.close();

        // There is no locking, so continue aborting active uploads until there are none left
        int abortedCount = -1;
        while (abortedCount != 0) {
            abortedCount = 0;
            Iterator<ActiveUpload> activeUploadIterator = _activeUploads.iterator();

            while (activeUploadIterator.hasNext()) {
                ActiveUpload upload = activeUploadIterator.next();
                abortedCount += 1;
                try {
                    upload.abort();
                } catch (Throwable t) {
                    // If we fail to stop an existing upload log it but otherwise move on; worse case is that the
                    // file gets transferred anyway.
                    _log.warn("Attempt to cancel active upload failed while closing S3ScanWriter: id={}", _taskId, t);
                } finally {
                    activeUploadIterator.remove();
                }
            }
        }
    }
}
